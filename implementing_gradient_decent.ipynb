{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3p+L/fM6V7YjZqvsswZ2n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yasmine-mk/implementing_linear_regression/blob/main/implementing_gradient_decent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "M1AFp2Ptus9z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import copy , math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"toc_15456_5\"></a>\n",
        "# implementing Predict Function \n",
        "The model's prediction with multiple variables is given by the linear model:\n",
        "\n",
        "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b $$\n",
        "or in vector notation:\n",
        "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  $$ \n",
        "where $\\cdot$ is a vector `dot product`\n",
        "here is an implementation of the predict function"
      ],
      "metadata": {
        "id": "WAP6_0CKyxtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predicting one vector \n",
        "# x here is an array of shape (n,) n referring to the number of features in our dataset\n",
        "def predict(x,w,b):\n",
        "  res = np.dot(x,w)+b\n",
        "  return res"
      ],
      "metadata": {
        "id": "UvOzUUNzx3gQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"toc_15456_5\"></a>\n",
        "# implementing Cost Function\n",
        "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
        "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 $$ \n",
        "where:\n",
        "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b   $$ "
      ],
      "metadata": {
        "id": "kv_V40MyvQde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute cost \n",
        "\"\"\"\n",
        "X is our traing data of shape (m,n)\n",
        "Y us the target data of shape (m,)\n",
        "w is our parameter (weiths) vector\n",
        "b is a scalar parameter (bias term) of shape ()\n",
        "\"\"\"\n",
        "def compute_cost(X,Y,w,b):\n",
        "  m = X.shape[0] #number of training examples\n",
        "  cost = 0.0     #cost\n",
        "  for i in range(m):\n",
        "    cost= cost + (np.dot(w, X[i]) +b - Y[i])**2     \n",
        "  cost = cost/(2*m) \n",
        "  return "
      ],
      "metadata": {
        "id": "B0rChYWjuzwP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"toc_15456_5\"></a>\n",
        "# implementing Gradient Descent \n",
        "the algorithm of radient descent for multiple variables is as follows:\n",
        "\n",
        "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
        "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  \\; & \\text{for j = 0..n-1}\\newline\n",
        "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
        "\\end{align*}$$\n",
        "\n",
        "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and   \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}   \\\\\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \n",
        "\\end{align}\n",
        "$$where:\n",
        "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b   \n",
        "$$ \n",
        "\n",
        "* m is the number of training examples in the data set\n",
        "\n",
        "    \n",
        "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the true (target) value\n",
        "\n",
        "\n",
        "PS : the implementation will be devided into two functions:\n",
        "\n",
        " * compute_gradient() : to compute the derivatives (inner loop) \n",
        "\n",
        " * gradient_decent() : to compute the outer loop"
      ],
      "metadata": {
        "id": "hk_1GiMc1vQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(X,Y,w,b):\n",
        "  m,n = X.shape\n",
        "  dj_dw = np.zeros((n,))\n",
        "  dj_db = 0.\n",
        "  for i in range(m):\n",
        "    err = np.dot(X[i],w)+b - Y[i]\n",
        "    for j in range (n):\n",
        "      dj_dw[j] = dj_dw[j] + sum* X[i,j]\n",
        "    dj_db = dj_db + err\n",
        "  dj_db = dj_db/m\n",
        "  dj_dw = dj_dw/m\n",
        "  return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "2CUy_9Z8u6mz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_decent(X, y, w_in, b_in,cost_function, gradient_function, learning_rate, num_iter):\n",
        "  # An array to store cost J and w's at each iteration primarily for graphing later\n",
        "  J_history = []\n",
        "  w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
        "  b = b_in\n",
        "  for i in range (num_iter):\n",
        "    # Calculate the gradient \n",
        "    dj_db, dj_dw = gradient_function(X,y,w,b)\n",
        "    # Update Parameters\n",
        "    w = w - learning_rate*dj_dw\n",
        "    b = b - learning_rate*dj_db\n",
        "    # Save cost J at each iteration\n",
        "    if i<100000:      # prevent resource exhaustion \n",
        "      J_history.append(cost_function(X, y, w, b))\n",
        "      # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "    if i% math.ceil(num_iter / 10) == 0:\n",
        "      print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
        "        \n",
        "  return w, b, J_history #return final w,b and J history for graphing"
      ],
      "metadata": {
        "id": "uwLIP3Hl_kq8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "miHRc_fIDHg-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}